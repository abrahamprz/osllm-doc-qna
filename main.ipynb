{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUICKSTART ([source](https://python.langchain.com/docs/use_cases/question_answering/quickstart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD THE DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in ./.venv/lib/python3.10/site-packages (4.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"docs/caperucitaroja.pdf\")\n",
    "docs = loader.load()\n",
    "pages = loader.load_and_split()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLIT DOCUMENT INTO CHUNCKS FOR EMBEDDING AND VECTOR STORAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In this case we’ll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\" [source](https://python.langchain.com/docs/use_cases/question_answering/quickstart#indexing-split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    # add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We need to index our chunks so we can search over them at runtime. The most common way to do this is to embed the contents of each document split and insert these embeddings into a vector database (or vector store). When we want to search over our splits, we take a text search query, embed it, and perform some sort of “similarity” search to identify the stored splits with the most similar embeddings to our query embedding. The simplest similarity measure is cosine similarity — we measure the cosine of the angle between each pair of embeddings (which are high dimensional vectors).\" [source](https://python.langchain.com/docs/use_cases/question_answering/quickstart#indexing-store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import (\n",
    "    Chroma,\n",
    ")  # Options: https://python.langchain.com/docs/integrations/vectorstores\n",
    "\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import (\n",
    "    GPT4AllEmbeddings,\n",
    ")  # Replaces the OpenAI option\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs = retriever.invoke(\"Como se llama la protagonista?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La abuela vivía en el bosque, a media hora de camino desde el \n",
      "pueblo. Apenas Caperucita Roja entró en el bosque, salió a su \n",
      "encuentro un lobo. Nunca antes la niña había visto a un lobo y \n",
      "desconocía lo peligroso que es ese animal. \n",
      "El lobo, con su voz más amistosa, le dijo:\n",
      "– ¡Buenos días, dulce pequeña! ¿Cómo te llamas?\n",
      "– ¡Buenos días! Me llaman Caperucita Roja.\n",
      "– ¿A dónde vas tan temprano?\n",
      "– A ver a mi abuelita.\n",
      "Caperucita roja.indd   6 17/09/2014   11:42:00\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Let’s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output.\" [source](https://python.langchain.com/docs/use_cases/question_answering/quickstart#retrieval-and-generation-generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import GPT4All\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=\"models/mistral-7b-openorca.gguf2.Q4_0.gguf\",  # https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf\n",
    "    max_tokens=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original prompt ([source](https://python.langchain.com/docs/use_cases/question_answering/quickstart#retrieval-and-generation-generate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import hub\n",
    "\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New prompt ([source](https://python.langchain.com/docs/use_cases/question_answering/quickstart#retrieval-and-generation-generate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Eres un asistente para responder preguntas. \\n    Utiliza los siguientes fragmentos de contexto recuperado para responder la pregunta. \\n    Si no conoces la respuesta, simplemente di que no la sabes. \\n    Usa máximo tres oraciones y mantén la respuesta concisa.\\n    Pregunta: pregunta ejemplo\\n    Contexto: contexto ejemplo\\n    Respuesta:\\n    '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Eres un asistente para responder preguntas. \n",
    "    Utiliza los siguientes fragmentos de contexto recuperado para responder la pregunta. \n",
    "    Si no conoces la respuesta, simplemente di que no la sabes. \n",
    "    Usa máximo tres oraciones y mantén la respuesta concisa.\n",
    "    Pregunta: {question}\n",
    "    Contexto: {context}\n",
    "    Respuesta:\n",
    "    \"\"\"\n",
    ")\n",
    "prompt_template.format(question=\"pregunta ejemplo\", context=\"contexto ejemplo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"context\": retriever | format_docs}\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Caperucita Roja iba a visitar a su abuela enferma y débil.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"A quien iba a visitar caperucita roja?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Caperucita Roja iba a visitar a su abuela enferma y débil."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"A quien iba a visitar caperucita roja?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' La pregunta que le hizo el lobo disfrazado de su abuela a Caperucita Roja fue \"¿Cómo te llamas?\" y luego preguntó \"¿A dónde vas tan temprano?\".'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\n",
    "    \"Que preguntas le hizo caperucita roja al lobo disfrazado de su abuela?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " La pregunta que le hizo el lobo disfrazado de su abuela a Caperucita Roja fue \"¿Cómo te llamas?\" y luego preguntó \"¿A dónde vas tan temprano?\"."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\n",
    "    \"Que preguntas le hizo caperucita roja al lobo disfrazado de su abuela?\"\n",
    "):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Sí, el lobo se comió a la abuela de Caperucita Roja en la versión original del cuento.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\n",
    "    \"El lobo se comio a la abuela de caperucita roja?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' En esta versión del cuento, el lobo intenta engañar y comer a Caperucita Roja pero es desmantelado por la niña y su abuela. El lobo se mete en la casa de la abuela para comérsela, pero al final es derrotado y no vuelve a aparecer.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\n",
    "    \"Que pasa con el lobo en esta version del cuento?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETURNING SOURCES ([source](https://python.langchain.com/docs/use_cases/question_answering/sources))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADDING SOURCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With LCEL it's easy to return the retrieved documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"question\": RunnablePassthrough(), \"context\": retriever}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Que pasa con el lobo en esta version del cuento?',\n",
       " 'context': [Document(page_content='El lobo giró el picaporte. La puerta se abrió. Sin pronunciar \\npalabra, fue directamente a la cama donde yacía la abuela y se \\nla tragó de un solo bocado. Entonces, se puso sus ropas, se calzó \\nsu cofia, cerró las cortinas y se metió en la cama.\\nCaperucita roja.indd   14 17/09/2014   11:42:26', metadata={'page': 14, 'source': 'docs/caperucitaroja.pdf'}),\n",
       "  Document(page_content='El lobo, después de haber saciado su apetito, se metió de nuevo \\nen la cama y se durmió. \\nUn rato después, un cazador pasó por delante de la casa y oyó \\nlos ronquidos. Se preocupó... \\n“La abuela ronca pero nunca tan fuerte. Miraré, no sea que le \\npase algo”.\\nY entró en la alcoba.\\nCaperucita roja.indd   20 17/09/2014   11:42:44', metadata={'page': 20, 'source': 'docs/caperucitaroja.pdf'}),\n",
       "  Document(page_content='El lobo pensó: “Esa joven y delicada niña será un suculento \\nbocado. Sabrá mucho mejor que la vieja. Haz de comportarte \\ncon astucia si quieres pescar a las dos”.\\nEntonces, acompañó un rato a la pequeña y luego le dijo:\\n–Caperucita, mira esas hermosas flores que te rodean. \\nEscucha el canto de los pajarillos. ¡Es tan divertido\\ncorretear por el bosque!\\nCaperucita roja.indd   8 17/09/2014   11:42:06', metadata={'page': 8, 'source': 'docs/caperucitaroja.pdf'}),\n",
       "  Document(page_content='SE CUENTA TAMBIÉN QUE... \\nEn cierta ocasión, cuando Caperucita Roja llevaba dulces a su \\nabuela, otro lobo se acercó a ella, le habló y quiso apartarla \\ndel camino. Pero esta vez Caperucita Roja se cuidó mucho de \\nhacerle caso. Siguió derechamente su camino y apenas llegó a la \\ncasa de la abuela le contó que se había encontrado con el lobo y \\nque éste le había dado los buenos días. \\n−Estoy segura –dijo la niña− que si me hubiera apartado \\ndel camino me hubiese devorado.\\n \\n−Ven conmigo –propuso la abuela−, vamos a cerrar la \\npuerta para que no pueda entrar. \\nCaperucita roja.indd   26 17/09/2014   11:43:00', metadata={'page': 26, 'source': 'docs/caperucitaroja.pdf'}),\n",
       "  Document(page_content='Un momento más tarde el lobo se despertó. Quiso dar un salto \\npara salir corriendo pero el peso de las piedras lo hizo caer. Se \\narrastró hasta la puerta y salió. Así se internó en el bosque y \\nnunca más se lo vio. \\nEn la casa de la abuela todo fue felicidad. Comieron la tarta, \\nbebieron la leche y festejaron con el cazador que ambas estuvieran \\nsanas y salvas.\\nFIN\\nCaperucita roja.indd   24 17/09/2014   11:42:55', metadata={'page': 24, 'source': 'docs/caperucitaroja.pdf'}),\n",
       "  Document(page_content='La abuela vivía en el bosque, a media hora de camino desde el \\npueblo. Apenas Caperucita Roja entró en el bosque, salió a su \\nencuentro un lobo. Nunca antes la niña había visto a un lobo y \\ndesconocía lo peligroso que es ese animal. \\nEl lobo, con su voz más amistosa, le dijo:\\n– ¡Buenos días, dulce pequeña! ¿Cómo te llamas?\\n– ¡Buenos días! Me llaman Caperucita Roja.\\n– ¿A dónde vas tan temprano?\\n– A ver a mi abuelita.\\nCaperucita roja.indd   6 17/09/2014   11:42:00', metadata={'page': 6, 'source': 'docs/caperucitaroja.pdf'})],\n",
       " 'answer': ' En esta versión del cuento, el lobo intenta engañar y comer a Caperucita Roja pero es desmantelado por la niña y su abuela. El lobo se mete en la casa de la abuela para comérsela, pero al final es derrotado y no vuelve a aparecer.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain_with_source.invoke(\n",
    "    \"Que pasa con el lobo en esta version del cuento?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD CHAT HISTORY ([source](https://python.langchain.com/docs/use_cases/question_answering/chat_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allows the user to have a back-and-forth conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In this guide we focus on adding logic for incorporating historical messages, and NOT on chat history management. Chat history management is [covered here](https://python.langchain.com/docs/expression_language/how_to/message_history).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to update two things about our existing app:\n",
    "- **Prompt:** Add support to historical messages as an input.\n",
    "- **Contextualizing questions:** Add a sub-chain that takes the latest user question and reformulates it in the context of the chat history. This is needed in case the latest question references some context from past messages. For example, if a user asks a follow-up question like “Can you elaborate on the second point?”, this cannot be understood without the context of the previous message. Therefore we can’t effectively perform retrieval with a question like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextualizing the question ([source](https://python.langchain.com/docs/use_cases/question_answering/chat_history#contextualizing-the-question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Se proporciona un historial de chat y la última pregunta \\\n",
    "    del usuario, la cual podría referirse al contexto del historial. Formula una pregunta independiente \\\n",
    "    que se pueda entender sin el historial. NO respondas la pregunta, solo reformúlala si es necesario, \\\n",
    "    de lo contrario, devuélvela tal cual.\n",
    "\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this chain we can ask follow-up questions that reference past messages and have them reformulated into standalone questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAI: Desmantelar significa desmontar o romper algo. En este caso, Caperucita Roja y su abuela logran desmantelar los planes del lobo al enfrentarse a él y protegerse mutuamente.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "contextualize_q_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"Que pasa con el lobo en esta version del cuento?\"),\n",
    "            AIMessage(content=\"En esta versión del cuento, el lobo intenta engañar y comer a Caperucita Roja pero es desmantelado por la niña y su abuela. El lobo se mete en la casa de la abuela para comérsela, pero al final es derrotado y no vuelve a aparecer.\"),\n",
    "        ],\n",
    "        \"question\": \"A que te refieres con desmantelado?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain with chat history ([source](https://python.langchain.com/docs/use_cases/question_answering/chat_history#chain-with-chat-history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"Eres un asistente para responder preguntas. \\\n",
    "Utiliza la siguiente información del contexto para responder la pregunta. \\\n",
    "Si no sabes la respuesta, simplemente di que no la sabes. \\\n",
    "Intenta mantener la respuesta concisa y usar máximo tres oraciones. \\\n",
    "{context}\n",
    "\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=contextualized_question | retriever | format_docs\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAsistente: En esta versión del cuento, el lobo intenta engañar a Caperucita Roja y a su abuela para comerlas. Sin embargo, la niña se cuida mucho de no hacerle caso al lobo y le cuenta a su abuela lo que pasó. La abuela cierra la puerta para protegerse del lobo, quien finalmente es derrotado en el bosque.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"Que pasa con el lobo en esta version del cuento?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAsistente: El lobo fue derrotado cuando Caperucita Roja y su abuelita lo atraparon en una situación donde él intentó comer a la niña, pero cayendo accidentalmente en un pozo de agua cerca. La pequeña se llevó el crédito por salvarla a ambas, aunque fue gracias a las acciones de su abuela que pudo derrotar al lobo.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "\n",
    "second_question = \"Como derrotaron al lobo?\"\n",
    "rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
