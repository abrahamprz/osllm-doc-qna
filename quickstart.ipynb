{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD THE DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in ./.venv/lib/python3.10/site-packages (4.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"docs/ciberseguridad.pdf\")\n",
    "docs = loader.load()\n",
    "pages = loader.load_and_split()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLIT DOCUMENT INTO CHUNCKS FOR EMBEDDING AND VECTOR STORAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In this case we’ll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\" [source](https://python.langchain.com/docs/use_cases/question_answering/quickstart#indexing-split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200, \n",
    "    # add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We need to index our chunks so we can search over them at runtime. The most common way to do this is to embed the contents of each document split and insert these embeddings into a vector database (or vector store). When we want to search over our splits, we take a text search query, embed it, and perform some sort of “similarity” search to identify the stored splits with the most similar embeddings to our query embedding. The simplest similarity measure is cosine similarity — we measure the cosine of the angle between each pair of embeddings (which are high dimensional vectors).\" [source](https://python.langchain.com/docs/use_cases/question_answering/quickstart#indexing-store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma # Options: https://python.langchain.com/docs/integrations/vectorstores\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings # Replaces the OpenAI option\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs = retriever.invoke(\"Cual es la teoria de la guerra y la comprension constructivista?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En este sentido, un elemento clave del \n",
      "neorrealismo es su consideración del \n",
      "papel vital de las empresas privadas, \n",
      "promotoras o creadores de softwares, \n",
      "como entidades de suma importancia \n",
      "para la consolidación de un régimen \n",
      "internacional de normas del Internet, \n",
      "así como para salvaguardar la seguri -\n",
      "dad nacional y delimitar la política \n",
      "exterior de las naciones. No obstante, \n",
      "deja en claro que los Estados-Nación \n",
      "son los mandamases de la regulación \n",
      "y control del ciberespacio (Nye, 2010).\n",
      "B.- La teoría de la guerra y la \n",
      "comprensión constructivista\n",
      "El desarrollo de la Teoría de la Gue -\n",
      "rra Moderna, de Carl Clausewitz, ha \n",
      "marcado un fuerte énfasis en las carac -\n",
      "terísticas de los campos o espacios de \n",
      "batalla, como un hecho crucial que de -\n",
      "termina la superioridad de un Estado, \n",
      "sobre otro en una confrontación bé -\n",
      "lica. En su obra clásica “De la Guerra” , \n",
      "Clausewitz delínea los conceptos clave \n",
      "de las estrategias castrenses del mun -\n",
      "do contemporáneo, a la par que en su\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Let’s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output.\" [source](https://python.langchain.com/docs/use_cases/question_answering/quickstart#retrieval-and-generation-generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import GPT4All\n",
    "\n",
    "llm = GPT4All(\n",
    "    model='/home/abraham/personal/osllm-doc-qna/mistral-7b-openorca.gguf2.Q4_0.gguf',\n",
    "    max_tokens=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original prompt ([source](https://python.langchain.com/docs/use_cases/question_answering/quickstart#retrieval-and-generation-generate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import hub\n",
    "\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New prompt ([source](https://python.langchain.com/docs/use_cases/question_answering/quickstart#retrieval-and-generation-generate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Eres un asistente para responder preguntas. \\n    Utiliza los siguientes fragmentos de contexto recuperado para responder la pregunta. \\n    Si no conoces la respuesta, simplemente di que no la sabes. \\n    Usa máximo tres oraciones y mantén la respuesta concisa.\\n    Pregunta: Pregunta ejemplo\\n    Contexto: respuesta ejemplo\\n    Respuesta:\\n    '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Eres un asistente para responder preguntas. \n",
    "    Utiliza los siguientes fragmentos de contexto recuperado para responder la pregunta. \n",
    "    Si no conoces la respuesta, simplemente di que no la sabes. \n",
    "    Usa máximo tres oraciones y mantén la respuesta concisa.\n",
    "    Pregunta: {question}\n",
    "    Contexto: {context}\n",
    "    Respuesta:\n",
    "    \"\"\"\n",
    ")\n",
    "prompt_template.format(question='Pregunta ejemplo', context='respuesta ejemplo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_messages = prompt_template.invoke"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
